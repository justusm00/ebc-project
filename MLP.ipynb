{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to predict missing Flux-data for GÃ¶ttingen forest and the forst bothanical garden.  \n",
    "The approach used is an MLP. Be careful to adjust how many cpu cores u want to use during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important  imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fastprogress\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 CPUs available, 4 were assigned\n"
     ]
    }
   ],
   "source": [
    "# Get number of cpus to use for faster parallelized data loading\n",
    "avb_cpus = os.cpu_count()\n",
    "\n",
    "num_cpus = 4\n",
    "\n",
    "print(avb_cpus, 'CPUs available,', num_cpus, 'were assigned' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# UTILITIES ############\n",
    "\n",
    "from modules.util import EBCDataset, grab_data, train_val_test_splitter, data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# TRAINING FUNCTIONS ###############\n",
    "\n",
    "# Define validation metric\n",
    "def prediction_error(y, y_pred): \n",
    "    #return abs(y - y_pred)\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer, model, master_bar, loss_fn = nn.MSELoss()):\n",
    "    \"\"\"Run one training epoch.\n",
    "\n",
    "    Args:\n",
    "        dataloade: dataloader containing trainingdata\n",
    "        optimizer: Torch optimizer object\n",
    "        model: the model that is trained\n",
    "        loss_fn: the loss function to be used -> nn.MSELoss()\n",
    "        master_bar: Will be iterated over for each\n",
    "            epoch to draw batches and display training progress\n",
    "\n",
    "    Returns:\n",
    "        Mean epoch loss and accuracy\n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    total_prediction_error = 0\n",
    "\n",
    "    for x, y in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
    "        # Reset optimmizers\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # For calculating the prediction error, add the distance between y and y_pred\n",
    "        # to the total error\n",
    "        total_prediction_error += prediction_error(y, y_pred)\n",
    "\n",
    "        # Compute loss\n",
    "        epoch_loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # For plotting the train loss, save it for each sample\n",
    "        loss.append(epoch_loss.item())\n",
    "\n",
    "    # Return the mean loss and the accuracy of this epoch\n",
    "    return np.mean(loss), total_prediction_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(dataloader, model, master_bar, loss_fn=nn.MSELoss()):\n",
    "    \"\"\"Compute loss and total prediction error on validation set.\n",
    "\n",
    "    Args:\n",
    "        dataloader: dataloader containing validation data\n",
    "        model (nn.Module): the model to train\n",
    "        loss_fn: the loss function to be used, defaults to MSELoss\n",
    "        master_bar (fastprogress.master_bar): Will be iterated over to draw \n",
    "            batches and show validation progress\n",
    "\n",
    "    Returns:\n",
    "        Mean loss and total prediction error on validation set\n",
    "    \"\"\"\n",
    "    epoch_loss = []\n",
    "    total_prediction_error = 0  \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
    "            # make a prediction on validation set\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # For calculating the prediction error, add the distance between y and y_pred\n",
    "            # to the total error\n",
    "            total_prediction_error += prediction_error(y, y_pred)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            # For plotting the train loss, save it for each sample\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    # Return the mean loss, the accuracy and the confusion matrix\n",
    "    return np.mean(epoch_loss), total_prediction_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot(title, label, train_results, val_results, yscale='linear', save_path=None):\n",
    "    \"\"\"Plot learning curves.\n",
    "\n",
    "    Args:\n",
    "        title: Title of plot\n",
    "        label: y-axis label\n",
    "        train_results: Vector containing training results over epochs\n",
    "        val_results: vector containing validation results over epochs\n",
    "        yscale: Defines how the y-axis scales\n",
    "        save_path: Optional path for saving file\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = np.arange(len(train_results)) + 1\n",
    "    \n",
    "    sns.set(style='ticks')\n",
    "\n",
    "    plt.plot(epochs, train_results, epochs, val_results, linestyle='dashed', marker='o')\n",
    "    legend = ['Train results', 'Validation results']\n",
    "        \n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.yscale(yscale)\n",
    "    plt.title(title)\n",
    "    \n",
    "    sns.despine(trim=True, offset=5)\n",
    "    plt.title(title, fontsize=15)\n",
    "    if save_path:\n",
    "        plt.savefig(str(os.path.join( save_path , label+\".png\")), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_training(model, optimizer, num_epochs, train_dataloader, val_dataloader, \n",
    "                 loss_fn=nn.MSELoss(), verbose=False):\n",
    "    \"\"\"Run model training.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be trained\n",
    "        optimizer: The optimizer used during training\n",
    "        loss_fn: Torch loss function for training -> nn.MSELoss()\n",
    "        num_epochs: How many epochs the model is trained for\n",
    "        train_dataloader:  dataloader containing training data\n",
    "        val_dataloader: dataloader containing validation data\n",
    "        verbose: Whether to print information on training progress\n",
    "\n",
    "    Returns:\n",
    "        lists containing  losses and total prediction errors per epoch for training and validation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    master_bar = fastprogress.master_bar(range(num_epochs))\n",
    "    train_losses, val_losses, train_tpes, val_tpes = [],[],[],[]\n",
    "\n",
    "    for epoch in master_bar:\n",
    "        # Train the model\n",
    "        epoch_train_loss, epoch_train_tpe = train(train_dataloader, optimizer, model, \n",
    "                                                 master_bar, loss_fn)\n",
    "        # Validate the model\n",
    "        epoch_val_loss, epoch_val_tpe = validate(val_dataloader, model, master_bar, loss_fn)\n",
    "\n",
    "        # Save loss and acc for plotting\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_tpes.append(epoch_train_tpe)\n",
    "        val_tpes.append(epoch_val_tpe)\n",
    "        \n",
    "        if verbose:\n",
    "            master_bar.write(f'Train loss: {epoch_train_loss:.2f}, val loss: {epoch_val_loss:.2f}, train acc: {epoch_train_tpe:.3f}, val acc {epoch_val_tpe:.3f}')\n",
    "\n",
    "    time_elapsed = np.round(time.time() - start_time, 0).astype(int)\n",
    "    print(f'Finished training after {time_elapsed} seconds.')\n",
    "\n",
    "    plot(\"Loss\", \"Loss\", train_losses, val_losses)\n",
    "    plot(\"TPE\", \"TPE\", train_tpes, val_tpes)\n",
    "\n",
    "    return train_losses, val_losses, train_tpes, val_tpes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.models import MLP\n",
    "from params import COLS_METEO, COLS_FLUXES, COLS_FEATURES, COLS_LABELS, PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data and create data loaders\n",
    "# if COLS_METEO and COLS_FLUXES are changed make sure to run the preprocessing pipeline first\n",
    "dataset, num_data, num_labels = grab_data('data/data_merged_without_nans.csv', COLS_FEATURES, COLS_LABELS)\n",
    "\n",
    "trainset, valset, testset = train_val_test_splitter(dataset)\n",
    "\n",
    "\n",
    "# normalize features based on trainset statistics only\n",
    "trainset_mean = torch.mean(trainset.dataset.dataset.data, dim=0)\n",
    "trainset_std = torch.std(trainset.dataset.dataset.data, dim=0)\n",
    "\n",
    "trainset.dataset.dataset.data = (trainset.dataset.dataset.data - trainset_mean) / trainset_std\n",
    "valset.dataset.dataset.data = (valset.dataset.dataset.data - trainset_mean) / trainset_std\n",
    "testset.dataset.data = (trainset.dataset.dataset.data - trainset_mean) / trainset_std\n",
    "\n",
    "\n",
    "\n",
    "trainloader, valloader, testloader = data_loaders(trainset, valset, testset, num_cpus=num_cpus, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should one model per location be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='39' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      78.00% [39/50 03:46&lt;01:03]\n",
       "    </div>\n",
       "    \n",
       "Train loss: 6035.37, val loss: 5183.47, train acc: 0.000, val acc 0.000<p>Train loss: 5811.22, val loss: 5289.78, train acc: 0.000, val acc 0.000<p>Train loss: 5814.36, val loss: 5183.93, train acc: 0.000, val acc 0.000<p>Train loss: 5811.97, val loss: 5167.24, train acc: 0.000, val acc 0.000<p>Train loss: 5801.62, val loss: 5305.16, train acc: 0.000, val acc 0.000<p>Train loss: 5801.54, val loss: 5145.58, train acc: 0.000, val acc 0.000<p>Train loss: 5811.45, val loss: 5227.80, train acc: 0.000, val acc 0.000<p>Train loss: 5814.64, val loss: 5144.10, train acc: 0.000, val acc 0.000<p>Train loss: 5813.30, val loss: 5202.75, train acc: 0.000, val acc 0.000<p>Train loss: 5811.68, val loss: 5144.03, train acc: 0.000, val acc 0.000<p>Train loss: 5805.57, val loss: 5151.58, train acc: 0.000, val acc 0.000<p>Train loss: 5805.07, val loss: 5146.27, train acc: 0.000, val acc 0.000<p>Train loss: 5803.99, val loss: 5167.25, train acc: 0.000, val acc 0.000<p>Train loss: 5796.15, val loss: 5199.39, train acc: 0.000, val acc 0.000<p>Train loss: 5807.30, val loss: 5187.69, train acc: 0.000, val acc 0.000<p>Train loss: 5796.50, val loss: 5141.17, train acc: 0.000, val acc 0.000<p>Train loss: 5798.06, val loss: 5140.36, train acc: 0.000, val acc 0.000<p>Train loss: 5803.84, val loss: 5129.19, train acc: 0.000, val acc 0.000<p>Train loss: 5802.77, val loss: 5204.13, train acc: 0.000, val acc 0.000<p>Train loss: 5785.54, val loss: 5118.08, train acc: 0.000, val acc 0.000<p>Train loss: 5800.56, val loss: 5196.07, train acc: 0.000, val acc 0.000<p>Train loss: 5795.19, val loss: 5131.48, train acc: 0.000, val acc 0.000<p>Train loss: 5787.22, val loss: 5142.95, train acc: 0.000, val acc 0.000<p>Train loss: 5799.85, val loss: 5125.86, train acc: 0.000, val acc 0.000<p>Train loss: 5802.34, val loss: 5122.62, train acc: 0.000, val acc 0.000<p>Train loss: 5787.85, val loss: 5137.51, train acc: 0.000, val acc 0.000<p>Train loss: 5796.94, val loss: 5203.77, train acc: 0.000, val acc 0.000<p>Train loss: 5792.26, val loss: 5155.54, train acc: 0.000, val acc 0.000<p>Train loss: 5796.75, val loss: 5173.06, train acc: 0.000, val acc 0.000<p>Train loss: 5793.33, val loss: 5246.81, train acc: 0.000, val acc 0.000<p>Train loss: 5784.97, val loss: 5121.44, train acc: 0.000, val acc 0.000<p>Train loss: 5793.20, val loss: 5149.68, train acc: 0.000, val acc 0.000<p>Train loss: 5788.09, val loss: 5197.58, train acc: 0.000, val acc 0.000<p>Train loss: 5787.42, val loss: 5136.47, train acc: 0.000, val acc 0.000<p>Train loss: 5788.22, val loss: 5153.87, train acc: 0.000, val acc 0.000<p>Train loss: 5774.97, val loss: 5115.94, train acc: 0.000, val acc 0.000<p>Train loss: 5777.01, val loss: 5108.79, train acc: 0.000, val acc 0.000<p>Train loss: 5769.57, val loss: 5100.01, train acc: 0.000, val acc 0.000<p>Train loss: 5770.90, val loss: 5135.38, train acc: 0.000, val acc 0.000<p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/165 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 10\u001b[0m train_losses, val_losses, train_tpes, val_tpes \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 158\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, num_epochs, train_dataloader, val_dataloader, loss_fn, verbose)\u001b[0m\n\u001b[1;32m    155\u001b[0m epoch_train_loss, epoch_train_tpe \u001b[38;5;241m=\u001b[39m train(train_dataloader, optimizer, model, \n\u001b[1;32m    156\u001b[0m                                          master_bar, loss_fn)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Validate the model\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m epoch_val_loss, epoch_val_tpe \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Save loss and acc for plotting\u001b[39;00m\n\u001b[1;32m    161\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(dataloader, model, master_bar, loss_fn)\u001b[0m\n\u001b[1;32m     73\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfastprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaster_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# make a prediction on validation set\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# For calculating the prediction error, add the distance between y and y_pred\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# to the total error\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Desktop/ebc_project/ebc-project/lib/python3.11/site-packages/fastprogress/fastprogress.py:41\u001b[0m, in \u001b[0;36mProgressBar.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ebc_project/ebc-project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/ebc_project/ebc-project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ebc_project/ebc-project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/ebc_project/ebc-project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = MLP(num_data, num_labels, num_hidden_units=30, num_hidden_layers=4)\n",
    "# Set loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "lr = 10**(-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses, val_losses, train_tpes, val_tpes = run_training(model, optimizer, num_epochs, trainloader, valloader, \n",
    "                                                              criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model_saves/mlp_1.pth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
