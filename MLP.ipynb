{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important  imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 CPUs available\n"
     ]
    }
   ],
   "source": [
    "# Get number of cpus to use for faster parallelized data loading\n",
    "num_cpus = os.cpu_count()\n",
    "print(num_cpus, 'CPUs available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# UTILITIES ############\n",
    "\n",
    "# Define Dataset\n",
    "class EBCDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "def grab_data(num_cpus=1):\n",
    "    \"\"\"Loads data from data_dir\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory to store data\n",
    "        num_cpus (int, optional): Number of cpus that should be used to \n",
    "            preprocess data. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Returns datasets as Dataset class for GÃ¶ttingen forest and Bothanic Garden\n",
    "    \"\"\"\n",
    "    # Load the data from 2023 and 2024 into pandas\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    data2023_BoG = pd.read_csv(os.path.join( cwd, 'data_2023/Fluxes_H_LE_CO2/BoG/FBG_fluxes_30min_20230101_20230801.csv' ))\n",
    "    data2023_GoeWa = pd.read_csv(os.path.join( cwd, 'data_2023/Fluxes_H_LE_CO2/GoeWa/GoeW_fluxes_30min_20230101_20230801.csv' ))\n",
    "    data2024_BoG = pd.read_csv(os.path.join( cwd, 'data_2024/EddyCovarianceData/eng/FBG_fluxes_30min_20240401_20240608_eng.csv' ))\n",
    "    data2024_GoeWa = pd.read_csv( os.path.join( cwd, 'data_2024/EddyCovarianceData/eng/GoeW_fluxes_30min_20240401_20240608_eng.csv' ) )\n",
    "\n",
    "    # Select data and labels\n",
    "\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "    # BoG23_set = EBCDataset( , , transform=transform )\n",
    "    # BoG24_set = EBCDataset( , , transform=transform )\n",
    "    # GoeWa23_set = EBCDataset( , , transform=transform )\n",
    "    # GoeWa24_set = EBCDataset( , , transform=transform )\n",
    "\n",
    "    # Bog = torch.utils.data.ConcatDataset( [BoG23_set, BoG24_set] )\n",
    "    # GoeWa = torch.utils.data.ConcatDataset( [GoeWa23_set, GoeWa24_set] )\n",
    "\n",
    "    return BoG, GoeWa\n",
    "\n",
    "\n",
    "\n",
    "# dataset Splitter \n",
    "def train_val_test_splitter(dataset, split_seed=42, test_frac=0.2, val_frac = 0.2):\n",
    "    \"\"\" Splits given dataset into train, val and test datasets\n",
    "\n",
    "    Args:\n",
    "        dataset: the given dataset\n",
    "        split_seed: the seed used for the rng\n",
    "        test_frac: fraction of data used for testing\n",
    "        val_frac_ fraction of training data used for validation\n",
    "    \"\"\"\n",
    "    # Train Test Split\n",
    "    num_test_samples = np.ceil(test_frac * dataset.data.shape[0]).astype(int)\n",
    "    num_train_samples = dataset.data.shape[0] - num_test_samples\n",
    "    trainset, testset = torch.utils.data.random_split(dataset, \n",
    "                                                    (num_train_samples, num_test_samples), \n",
    "                                                    generator=torch.Generator().manual_seed(split_seed))\n",
    "    \n",
    "    # Train Val Split\n",
    "    num_val_samples = np.ceil(val_frac * trainset.data.shape[0]).astype(int)\n",
    "    num_train_samples = trainset.data.shape[0] - num_val_samples\n",
    "    trainset, valset = torch.utils.data.random_split(trainset, \n",
    "                                                    (num_train_samples, num_val_samples), \n",
    "                                                    generator=torch.Generator().manual_seed(split_seed))\n",
    "    \n",
    "    return trainset, valset, testset\n",
    "\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "def data_loaders(trainset, valset, testset, batch_size=64, num_cpus=1):\n",
    "    \"\"\"Initialize train, validation and test data loader.\n",
    "\n",
    "    Args:\n",
    "        trainset: Training set torchvision dataset object.\n",
    "        valset: Validation set torchvision dataset object.\n",
    "        testset: Test set torchvision dataset object.\n",
    "        batch_size: Batchsize used during training, defaults to 64\n",
    "        num_cpus: Number of CPUs to use when iterating over\n",
    "            the data loader. More is faster. Defaults to 1.\n",
    "    \"\"\"        \n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=num_cpus)\n",
    "    valloader = torch.utils.data.DataLoader(valset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=num_cpus)\n",
    "    testloader = torch.utils.data.DataLoader(testset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True, \n",
    "                                             num_workers=num_cpus)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# TRAINING FUNCTIONS ###############\n",
    "\n",
    "# Define validation metric\n",
    "def prediction_error(y, y_pred): \n",
    "    return abs(y - y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer, model, master_bar, loss_fn = nn.MSELoss()):\n",
    "    \"\"\"Run one training epoch.\n",
    "\n",
    "    Args:\n",
    "        dataloade: dataloader containing trainingdata\n",
    "        optimizer: Torch optimizer object\n",
    "        model: the model that is trained\n",
    "        loss_fn: the loss function to be used -> nn.MSELoss()\n",
    "        master_bar: Will be iterated over for each\n",
    "            epoch to draw batches and display training progress\n",
    "\n",
    "    Returns:\n",
    "        Mean epoch loss and accuracy\n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    total_prediction_error = 0\n",
    "\n",
    "    for x, y in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
    "        # Reset optimmizers\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # For calculating the prediction error, add the distance between y and y_pred\n",
    "        # to the total error\n",
    "        total_prediction_error += prediction_error(y, y_pred)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # For plotting the train loss, save it for each sample\n",
    "        loss.append(loss.item())\n",
    "\n",
    "    # Return the mean loss and the accuracy of this epoch\n",
    "    return np.mean(loss), total_prediction_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(dataloader, model, master_bar, loss_fn=nn.MSELoss()):\n",
    "    \"\"\"Compute loss and total prediction error on validation set.\n",
    "\n",
    "    Args:\n",
    "        dataloader: dataloader containing validation data\n",
    "        model (nn.Module): the model to train\n",
    "        loss_fn: the loss function to be used, defaults to MSELoss\n",
    "        master_bar (fastprogress.master_bar): Will be iterated over to draw \n",
    "            batches and show validation progress\n",
    "\n",
    "    Returns:\n",
    "        Mean loss and total prediction error on validation set\n",
    "    \"\"\"\n",
    "    epoch_loss = []\n",
    "    total_prediction_error = 0  \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
    "            # make a prediction on validation set\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # For calculating the prediction error, add the distance between y and y_pred\n",
    "            # to the total error\n",
    "            total_prediction_error += prediction_error(y, y_pred)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            # For plotting the train loss, save it for each sample\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    # Return the mean loss, the accuracy and the confusion matrix\n",
    "    return np.mean(epoch_loss), total_prediction_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot(title, label, train_results, val_results, yscale='linear', save_path=None):\n",
    "    \"\"\"Plot learning curves.\n",
    "\n",
    "    Args:\n",
    "        title: Title of plot\n",
    "        label: y-axis label\n",
    "        train_results: Vector containing training results over epochs\n",
    "        val_results: vector containing validation results over epochs\n",
    "        yscale: Defines how the y-axis scales\n",
    "        save_path: Optional path for saving file\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = np.arange(len(train_results)) + 1\n",
    "    \n",
    "    sns.set(style='ticks')\n",
    "\n",
    "    plt.plot(epochs, train_results, epochs, val_results, linestyle='dashed', marker='o')\n",
    "    legend = ['Train results', 'Validation results']\n",
    "        \n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.yscale(yscale)\n",
    "    plt.title(title)\n",
    "    \n",
    "    sns.despine(trim=True, offset=5)\n",
    "    plt.title(title, fontsize=15)\n",
    "    if save_path:\n",
    "        plt.savefig(str(os.path.join( save_path , label+\".png\")), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_training(model, optimizer, num_epochs, train_dataloader, val_dataloader, \n",
    "                 loss_fn=nn.MSELoss(), verbose=False):\n",
    "    \"\"\"Run model training.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be trained\n",
    "        optimizer: The optimizer used during training\n",
    "        loss_fn: Torch loss function for training -> nn.MSELoss()\n",
    "        num_epochs: How many epochs the model is trained for\n",
    "        train_dataloader:  dataloader containing training data\n",
    "        val_dataloader: dataloader containing validation data\n",
    "        verbose: Whether to print information on training progress\n",
    "\n",
    "    Returns:\n",
    "        lists containing  losses and total prediction errors per epoch for training and validation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    master_bar = fastprogress.master_bar(range(num_epochs))\n",
    "    train_losses, val_losses, train_tpes, val_tpes = [],[],[],[]\n",
    "\n",
    "    for epoch in master_bar:\n",
    "        # Train the model\n",
    "        epoch_train_loss, epoch_train_tpe = train(train_dataloader, optimizer, model, \n",
    "                                                  loss_fn, master_bar)\n",
    "        # Validate the model\n",
    "        epoch_val_loss, epoch_val_tpe = validate(val_dataloader, model, loss_fn, master_bar)\n",
    "\n",
    "        # Save loss and acc for plotting\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_tpes.append(epoch_train_tpe)\n",
    "        val_tpes.append(epoch_val_tpe)\n",
    "        \n",
    "        if verbose:\n",
    "            master_bar.write(f'Train loss: {epoch_train_loss:.2f}, val loss: {epoch_val_loss:.2f}, train acc: {epoch_train_tpe:.3f}, val acc {epoch_val_tpe:.3f}')\n",
    "\n",
    "    time_elapsed = np.round(time.time() - start_time, 0).astype(int)\n",
    "    print(f'Finished training after {time_elapsed} seconds.')\n",
    "\n",
    "    plot(\"Loss\", \"Loss\", train_losses, val_losses)\n",
    "    plot(\"TPE\", \"TPE\", train_tpes, val_tpes)\n",
    "\n",
    "    return train_losses, val_losses, train_tpes, val_tpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
